{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility.\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log generation configuration.\n",
    "NUM_NORMAL_LOGS = 95000  # Normal logs (includes expected errors).\n",
    "NUM_ANOMALOUS_LOGS = 5000  # True anomalous patterns (~5% anomaly rate).\n",
    "TOTAL_LOGS = NUM_NORMAL_LOGS + NUM_ANOMALOUS_LOGS\n",
    "\n",
    "# Services in the system.\n",
    "SERVICES = [\n",
    "    'auth-service',\n",
    "    'payment-service',\n",
    "    'order-service',\n",
    "    'inventory-service',\n",
    "    'user-service',\n",
    "    'notification-service',\n",
    "    'api-gateway'\n",
    "]\n",
    "\n",
    "# Log levels.\n",
    "LOG_LEVELS = ['INFO', 'DEBUG', 'WARN', 'ERROR', 'FATAL']\n",
    "\n",
    "# Normal log level distribution (includes some expected errors).\n",
    "NORMAL_LEVEL_WEIGHTS = [0.65, 0.20, 0.10, 0.045, 0.005]\n",
    "\n",
    "# Anomaly types.\n",
    "ANOMALY_TYPES = ['spike', 'cascade', 'new_pattern', 'resource_exhaustion', 'security_breach']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Log Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal log message templates (including EXPECTED errors that are NOT anomalies).\n",
    "NORMAL_TEMPLATES = {\n",
    "    'INFO': [\n",
    "        'Request processed successfully - user_id: {user_id}, duration: {duration}ms',\n",
    "        'Authentication successful for user {user_id}',\n",
    "        'Payment processed - transaction_id: {transaction_id}, amount: ${amount}',\n",
    "        'Order created successfully - order_id: {order_id}',\n",
    "        'Cache hit for key: {cache_key}',\n",
    "        'Database query completed - rows: {rows}, duration: {duration}ms',\n",
    "        'API request received - endpoint: {endpoint}, method: {method}',\n",
    "        'User session created - session_id: {session_id}',\n",
    "        'Email notification sent to {email}',\n",
    "        'Inventory updated - item_id: {item_id}, quantity: {quantity}',\n",
    "    ],\n",
    "    'DEBUG': [\n",
    "        'Entering function: {function_name}',\n",
    "        'Cache miss for key: {cache_key}',\n",
    "        'Query plan: {query_plan}',\n",
    "        'Response payload size: {size} bytes',\n",
    "        'Connection pool stats: active={active}, idle={idle}',\n",
    "    ],\n",
    "    'WARN': [\n",
    "        'High memory usage detected: {memory}%',\n",
    "        'Slow query detected - duration: {duration}ms',\n",
    "        'Rate limit approaching for user {user_id}',\n",
    "        'Connection retry attempt {attempt} of 3',\n",
    "        'Cache eviction due to size limit',\n",
    "        'Service latency above threshold: {duration}ms',\n",
    "    ],\n",
    "    # EXPECTED ERRORS (business logic, user errors - NOT anomalies).\n",
    "    'ERROR': [\n",
    "        'Authentication failed - invalid password for user {user_id}',\n",
    "        'Validation error - missing required field: {field_name}',\n",
    "        'Order failed - insufficient inventory for item {item_id}',\n",
    "        'Payment declined - insufficient funds for user {user_id}',\n",
    "        'Invalid request - malformed JSON in request body',\n",
    "        'Resource not found - user_id {user_id} does not exist',\n",
    "        'Session expired for user {user_id}',\n",
    "        'Rate limit exceeded for user {user_id} - retry after {retry_after}s',\n",
    "    ],\n",
    "    # RARE but expected critical issues (NOT anomalies when isolated).\n",
    "    'FATAL': [\n",
    "        'Unhandled exception in request handler - {exception_type}',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomalous Log Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalous log message templates (TRUE system anomalies).\n",
    "# These represent UNUSUAL patterns, not just error-level logs.\n",
    "ANOMALY_TEMPLATES = {\n",
    "    # Error spike patterns.\n",
    "    'spike_error': [\n",
    "        'Database connection pool exhausted - max connections: {max_conn}',\n",
    "        'Service unavailable - {service} not responding after {attempts} attempts',\n",
    "        'Connection timeout to {host} - {error_msg}',\n",
    "        'Failed to acquire database lock - deadlock detected',\n",
    "    ],\n",
    "    # Cascade failure patterns.\n",
    "    'cascade': [\n",
    "        'Circuit breaker opened for {service} - failure threshold exceeded',\n",
    "        'Upstream service {service} unavailable - cascading failure',\n",
    "        'Database cluster unreachable - all nodes down',\n",
    "        'Message queue full - dropping messages',\n",
    "    ],\n",
    "    # New/unusual error patterns.\n",
    "    'new_pattern': [\n",
    "        'NullPointerException in {function_name} at line {line}',\n",
    "        'OutOfMemoryError - heap space exceeded',\n",
    "        'StackOverflowError in {function_name}',\n",
    "        'Data corruption detected in table {table_name}',\n",
    "        'Unexpected error code {error_code} from external API',\n",
    "    ],\n",
    "    # Resource exhaustion.\n",
    "    'resource': [\n",
    "        'Disk space critically low: {disk_space}% remaining',\n",
    "        'Memory usage critical: {memory}% used',\n",
    "        'CPU usage sustained above 95% for {duration} seconds',\n",
    "        'File descriptor limit reached',\n",
    "        'Thread pool exhausted - queue size: {queue_size}',\n",
    "    ],\n",
    "    # Security anomalies.\n",
    "    'security': [\n",
    "        'Suspicious activity detected from IP: {ip_address}',\n",
    "        'Brute force attack detected - {attempts} failed login attempts',\n",
    "        'Potential SQL injection attempt in query',\n",
    "        'Unauthorized access attempt to admin endpoint',\n",
    "        'Token validation failed - possible token forgery',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timestamp_from_datetime(dt: datetime) -> str:\n",
    "    \"\"\"\n",
    "    Convert datetime to ISO format timestamp string.\n",
    "\n",
    "    param dt: Datetime object.\n",
    "    \"\"\"\n",
    "    return dt.isoformat() + 'Z'\n",
    "\n",
    "\n",
    "def generate_normal_log(timestamp: datetime, service: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generate a normal log entry (including expected business errors).\n",
    "\n",
    "    param timestamp: Log timestamp as datetime.\n",
    "    param service: Service name.\n",
    "    \"\"\"\n",
    "    level = random.choices(LOG_LEVELS, weights=NORMAL_LEVEL_WEIGHTS)[0]\n",
    "    template = random.choice(NORMAL_TEMPLATES[level])\n",
    "    \n",
    "    # Generate realistic field values.\n",
    "    message = template.format(\n",
    "        user_id=random.randint(1000, 9999),\n",
    "        duration=random.randint(10, 500),\n",
    "        transaction_id=f'txn_{random.randint(100000, 999999)}',\n",
    "        amount=round(random.uniform(10, 1000), 2),\n",
    "        order_id=f'ord_{random.randint(100000, 999999)}',\n",
    "        cache_key=f'cache_{random.randint(1, 100)}',\n",
    "        rows=random.randint(1, 1000),\n",
    "        endpoint=random.choice(['/api/users', '/api/orders', '/api/payments']),\n",
    "        method=random.choice(['GET', 'POST', 'PUT']),\n",
    "        session_id=f'sess_{random.randint(100000, 999999)}',\n",
    "        email=f'user{random.randint(1, 1000)}@example.com',\n",
    "        item_id=f'item_{random.randint(1, 500)}',\n",
    "        quantity=random.randint(1, 100),\n",
    "        function_name=random.choice(['processPayment', 'validateUser', 'updateInventory']),\n",
    "        query_plan='index_scan',\n",
    "        size=random.randint(100, 50000),\n",
    "        active=random.randint(5, 20),\n",
    "        idle=random.randint(10, 50),\n",
    "        memory=random.randint(40, 75),\n",
    "        attempt=random.randint(1, 3),\n",
    "        field_name=random.choice(['email', 'password', 'amount', 'item_id']),\n",
    "        retry_after=random.randint(30, 300),\n",
    "        exception_type=random.choice(['ValueError', 'TypeError', 'KeyError'])\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "        'level': level,\n",
    "        'service': service,\n",
    "        'message': message,\n",
    "        'is_anomaly': 0,\n",
    "        'anomaly_type': None\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_anomaly_spike(start_time: datetime, service: str, spike_size: int = 50) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate a spike of error logs (anomaly pattern).\n",
    "\n",
    "    param start_time: Starting timestamp.\n",
    "    param service: Service experiencing the spike.\n",
    "    param spike_size: Number of errors in the spike.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    anomaly_category = random.choice(['spike_error', 'cascade', 'resource'])\n",
    "    template = random.choice(ANOMALY_TEMPLATES[anomaly_category])\n",
    "    \n",
    "    for i in range(spike_size):\n",
    "        timestamp = start_time + timedelta(seconds=i * random.uniform(0.1, 2))\n",
    "        \n",
    "        message = template.format(\n",
    "            max_conn=random.choice([50, 100, 200]),\n",
    "            service=random.choice(SERVICES),\n",
    "            attempts=random.randint(3, 10),\n",
    "            host=f'db-{random.randint(1, 5)}.example.com',\n",
    "            error_msg=random.choice(['Connection refused', 'Timeout', 'Network unreachable']),\n",
    "            function_name=random.choice(['processPayment', 'validateUser', 'updateInventory']),\n",
    "            line=random.randint(100, 500),\n",
    "            table_name=random.choice(['users', 'orders', 'payments']),\n",
    "            error_code=f'E{random.randint(1000, 9999)}',\n",
    "            disk_space=random.randint(1, 5),\n",
    "            memory=random.randint(90, 99),\n",
    "            duration=random.randint(300, 600),\n",
    "            queue_size=random.randint(10000, 50000),\n",
    "            ip_address=f'{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}'\n",
    "        )\n",
    "        \n",
    "        logs.append({\n",
    "            'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "            'level': random.choice(['ERROR', 'FATAL', 'WARN']),\n",
    "            'service': service,\n",
    "            'message': message,\n",
    "            'is_anomaly': 1,\n",
    "            'anomaly_type': f'{anomaly_category}_spike'\n",
    "        })\n",
    "    \n",
    "    return logs\n",
    "\n",
    "\n",
    "def generate_cascade_failure(start_time: datetime, affected_services: list[str], cascade_size: int = 30) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate cascade failure across multiple services (anomaly pattern).\n",
    "\n",
    "    param start_time: Starting timestamp.\n",
    "    param affected_services: Services affected by cascade.\n",
    "    param cascade_size: Number of logs in cascade.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    templates = ANOMALY_TEMPLATES['cascade']\n",
    "    \n",
    "    for i in range(cascade_size):\n",
    "        timestamp = start_time + timedelta(seconds=i * random.uniform(1, 5))\n",
    "        service = random.choice(affected_services)\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        message = template.format(\n",
    "            service=random.choice(SERVICES)\n",
    "        )\n",
    "        \n",
    "        logs.append({\n",
    "            'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "            'level': random.choice(['ERROR', 'FATAL']),\n",
    "            'service': service,\n",
    "            'message': message,\n",
    "            'is_anomaly': 1,\n",
    "            'anomaly_type': 'cascade_failure'\n",
    "        })\n",
    "    \n",
    "    return logs\n",
    "\n",
    "\n",
    "def generate_new_pattern_anomaly(timestamp: datetime, service: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generate a new/unusual error pattern (anomaly).\n",
    "\n",
    "    param timestamp: Log timestamp.\n",
    "    param service: Service name.\n",
    "    \"\"\"\n",
    "    template = random.choice(ANOMALY_TEMPLATES['new_pattern'])\n",
    "    \n",
    "    message = template.format(\n",
    "        function_name=random.choice(['processPayment', 'validateUser', 'updateInventory']),\n",
    "        line=random.randint(50, 500),\n",
    "        table_name=random.choice(['users', 'orders', 'payments']),\n",
    "        error_code=f'E{random.randint(1000, 9999)}'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "        'level': random.choice(['ERROR', 'FATAL']),\n",
    "        'service': service,\n",
    "        'message': message,\n",
    "        'is_anomaly': 1,\n",
    "        'anomaly_type': 'new_pattern'\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_security_anomaly(start_time: datetime, num_events: int = 20) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate security-related anomaly (multiple suspicious events).\n",
    "\n",
    "    param start_time: Starting timestamp.\n",
    "    param num_events: Number of security events.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    ip = f'{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}'\n",
    "    \n",
    "    for i in range(num_events):\n",
    "        timestamp = start_time + timedelta(seconds=i * random.uniform(0.5, 3))\n",
    "        template = random.choice(ANOMALY_TEMPLATES['security'])\n",
    "        \n",
    "        message = template.format(\n",
    "            ip_address=ip,\n",
    "            attempts=i + 1\n",
    "        )\n",
    "        \n",
    "        logs.append({\n",
    "            'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "            'level': 'WARN' if i < num_events - 5 else 'ERROR',\n",
    "            'service': 'auth-service',\n",
    "            'message': message,\n",
    "            'is_anomaly': 1,\n",
    "            'anomaly_type': 'security_breach'\n",
    "        })\n",
    "    \n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(\n",
    "    num_normal: int = NUM_NORMAL_LOGS,\n",
    "    duration_minutes: int = 60,\n",
    "    num_spikes: int = 15,\n",
    "    num_cascades: int = 5,\n",
    "    num_new_patterns: int = 100,\n",
    "    num_security_incidents: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate realistic training data with normal logs and various anomaly patterns.\n",
    "\n",
    "    param num_normal: Number of normal log entries (baseline traffic).\n",
    "    param duration_minutes: Time window in minutes.\n",
    "    param num_spikes: Number of error spikes to inject.\n",
    "    param num_cascades: Number of cascade failures to inject.\n",
    "    param num_new_patterns: Number of new/unusual error patterns.\n",
    "    param num_security_incidents: Number of security incidents.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"REALISTIC LOG DATA GENERATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nGenerating logs over {duration_minutes} minute window...\")\n",
    "    print(f\"Normal logs (baseline): {num_normal:,}\")\n",
    "    print(f\"Anomaly patterns to inject:\")\n",
    "    print(f\"  - Error spikes: {num_spikes}\")\n",
    "    print(f\"  - Cascade failures: {num_cascades}\")\n",
    "    print(f\"  - New error patterns: {num_new_patterns}\")\n",
    "    print(f\"  - Security incidents: {num_security_incidents}\")\n",
    "    print()\n",
    "    \n",
    "    start_time = datetime.now() - timedelta(minutes=duration_minutes)\n",
    "    logs = []\n",
    "    \n",
    "    # Generate normal baseline logs.\n",
    "    print(\"Generating baseline traffic...\")\n",
    "    for i in range(num_normal):\n",
    "        # Calculate timestamp with realistic spacing.\n",
    "        avg_interval = (duration_minutes * 60) / num_normal\n",
    "        offset = i * avg_interval + random.uniform(-avg_interval * 0.3, avg_interval * 0.3)\n",
    "        timestamp = start_time + timedelta(seconds=offset)\n",
    "        \n",
    "        service = random.choice(SERVICES)\n",
    "        logs.append(generate_normal_log(timestamp, service))\n",
    "        \n",
    "        if (i + 1) % 20000 == 0:\n",
    "            print(f\"  Generated {i + 1:,} baseline logs...\")\n",
    "    \n",
    "    print(f\"✓ Baseline complete: {num_normal:,} logs\\n\")\n",
    "    \n",
    "    # Inject error spikes.\n",
    "    print(f\"Injecting {num_spikes} error spikes...\")\n",
    "    for i in range(num_spikes):\n",
    "        spike_time = start_time + timedelta(minutes=random.uniform(5, duration_minutes - 5))\n",
    "        service = random.choice(SERVICES)\n",
    "        spike_size = random.randint(30, 80)\n",
    "        spike_logs = generate_anomaly_spike(spike_time, service, spike_size)\n",
    "        logs.extend(spike_logs)\n",
    "        print(f\"  Spike {i+1}: {spike_size} errors at {spike_time.strftime('%H:%M:%S')} from {service}\")\n",
    "    \n",
    "    print(f\"✓ Spikes injected\\n\")\n",
    "    \n",
    "    # Inject cascade failures.\n",
    "    print(f\"Injecting {num_cascades} cascade failures...\")\n",
    "    for i in range(num_cascades):\n",
    "        cascade_time = start_time + timedelta(minutes=random.uniform(10, duration_minutes - 10))\n",
    "        affected = random.sample(SERVICES, k=random.randint(3, 5))\n",
    "        cascade_size = random.randint(40, 60)\n",
    "        cascade_logs = generate_cascade_failure(cascade_time, affected, cascade_size)\n",
    "        logs.extend(cascade_logs)\n",
    "        print(f\"  Cascade {i+1}: {cascade_size} errors across {len(affected)} services at {cascade_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    print(f\"✓ Cascades injected\\n\")\n",
    "    \n",
    "    # Inject new/unusual patterns.\n",
    "    print(f\"Injecting {num_new_patterns} new error patterns...\")\n",
    "    for i in range(num_new_patterns):\n",
    "        pattern_time = start_time + timedelta(minutes=random.uniform(0, duration_minutes))\n",
    "        service = random.choice(SERVICES)\n",
    "        logs.append(generate_new_pattern_anomaly(pattern_time, service))\n",
    "    \n",
    "    print(f\"✓ New patterns injected\\n\")\n",
    "    \n",
    "    # Inject security incidents.\n",
    "    print(f\"Injecting {num_security_incidents} security incidents...\")\n",
    "    for i in range(num_security_incidents):\n",
    "        incident_time = start_time + timedelta(minutes=random.uniform(5, duration_minutes - 5))\n",
    "        incident_logs = generate_security_anomaly(incident_time, num_events=random.randint(15, 30))\n",
    "        logs.extend(incident_logs)\n",
    "        print(f\"  Security incident {i+1} at {incident_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    print(f\"✓ Security incidents injected\\n\")\n",
    "    \n",
    "    # Create DataFrame and sort by timestamp.\n",
    "    print(\"Creating dataset...\")\n",
    "    df = pd.DataFrame(logs)\n",
    "    df['timestamp_dt'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp_dt').reset_index(drop=True)\n",
    "    df = df.drop('timestamp_dt', axis=1)\n",
    "    \n",
    "    # Calculate statistics.\n",
    "    total_logs = len(df)\n",
    "    anomalous_logs = df['is_anomaly'].sum()\n",
    "    anomaly_rate = (anomalous_logs / total_logs) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATASET GENERATED SUCCESSFULLY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total logs: {total_logs:,}\")\n",
    "    print(f\"Normal logs: {total_logs - anomalous_logs:,} ({100 - anomaly_rate:.2f}%)\")\n",
    "    print(f\"Anomalous logs: {anomalous_logs:,} ({anomaly_rate:.2f}%)\")\n",
    "    print(f\"\\nKey insight: Not all ERROR/FATAL logs are anomalies!\")\n",
    "    print(f\"  - Baseline contains {df[(df['is_anomaly'] == 0) & (df['level'].isin(['ERROR', 'FATAL']))].shape[0]:,} expected errors\")\n",
    "    print(f\"  - Anomalies are identified by PATTERNS (spikes, cascades, new errors)\")\n",
    "    print()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REALISTIC LOG DATA GENERATION\n",
      "============================================================\n",
      "\n",
      "Generating logs over 60 minute window...\n",
      "Normal logs (baseline): 95,000\n",
      "Anomaly patterns to inject:\n",
      "  - Error spikes: 15\n",
      "  - Cascade failures: 5\n",
      "  - New error patterns: 100\n",
      "  - Security incidents: 3\n",
      "\n",
      "Generating baseline traffic...\n",
      "  Generated 20,000 baseline logs...\n",
      "  Generated 40,000 baseline logs...\n",
      "  Generated 60,000 baseline logs...\n",
      "  Generated 80,000 baseline logs...\n",
      "✓ Baseline complete: 95,000 logs\n",
      "\n",
      "Injecting 15 error spikes...\n",
      "  Spike 1: 50 errors at 16:04:10 from api-gateway\n",
      "  Spike 2: 56 errors at 16:02:49 from user-service\n",
      "  Spike 3: 66 errors at 15:34:24 from payment-service\n",
      "  Spike 4: 42 errors at 16:16:11 from api-gateway\n",
      "  Spike 5: 80 errors at 15:33:04 from api-gateway\n",
      "  Spike 6: 72 errors at 15:34:58 from notification-service\n",
      "  Spike 7: 50 errors at 15:48:07 from order-service\n",
      "  Spike 8: 66 errors at 15:54:40 from api-gateway\n",
      "  Spike 9: 73 errors at 16:09:51 from auth-service\n",
      "  Spike 10: 32 errors at 15:56:47 from auth-service\n",
      "  Spike 11: 70 errors at 15:36:44 from api-gateway\n",
      "  Spike 12: 33 errors at 15:34:35 from user-service\n",
      "  Spike 13: 64 errors at 16:15:18 from payment-service\n",
      "  Spike 14: 48 errors at 15:28:08 from notification-service\n",
      "  Spike 15: 32 errors at 16:09:27 from user-service\n",
      "✓ Spikes injected\n",
      "\n",
      "Injecting 5 cascade failures...\n",
      "  Cascade 1: 41 errors across 4 services at 15:41:19\n",
      "  Cascade 2: 49 errors across 3 services at 16:04:24\n",
      "  Cascade 3: 56 errors across 5 services at 15:55:34\n",
      "  Cascade 4: 40 errors across 3 services at 15:33:49\n",
      "  Cascade 5: 46 errors across 5 services at 15:33:48\n",
      "✓ Cascades injected\n",
      "\n",
      "Injecting 100 new error patterns...\n",
      "✓ New patterns injected\n",
      "\n",
      "Injecting 3 security incidents...\n",
      "  Security incident 1 at 16:07:45\n",
      "  Security incident 2 at 16:11:36\n",
      "  Security incident 3 at 15:41:36\n",
      "✓ Security incidents injected\n",
      "\n",
      "Creating dataset...\n",
      "\n",
      "============================================================\n",
      "DATASET GENERATED SUCCESSFULLY\n",
      "============================================================\n",
      "Total logs: 96,237\n",
      "Normal logs: 95,000 (98.71%)\n",
      "Anomalous logs: 1,237 (1.29%)\n",
      "\n",
      "Key insight: Not all ERROR/FATAL logs are anomalies!\n",
      "  - Baseline contains 4,782 expected errors\n",
      "  - Anomalies are identified by PATTERNS (spikes, cascades, new errors)\n",
      "\n",
      "Shape: (96237, 6)\n",
      "Columns: ['timestamp', 'level', 'service', 'message', 'is_anomaly', 'anomaly_type']\n"
     ]
    }
   ],
   "source": [
    "# Generate the training data with realistic anomaly patterns.\n",
    "df = generate_training_data(\n",
    "    num_normal=NUM_NORMAL_LOGS,\n",
    "    duration_minutes=60,\n",
    "    num_spikes=15,\n",
    "    num_cascades=5,\n",
    "    num_new_patterns=100,\n",
    "    num_security_incidents=3\n",
    ")\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Sample Normal Logs:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>level</th>\n",
       "      <th>service</th>\n",
       "      <th>message</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>anomaly_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-24T15:22:22.408389Z</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>auth-service</td>\n",
       "      <td>Cache miss for key: cache_70</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-24T15:22:22.445143Z</td>\n",
       "      <td>WARN</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Slow query detected - duration: 184ms</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-24T15:22:22.476306Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>notification-service</td>\n",
       "      <td>Database query completed - rows: 297, duration...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-24T15:22:22.521972Z</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>user-service</td>\n",
       "      <td>Cache miss for key: cache_89</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-24T15:22:22.565355Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>order-service</td>\n",
       "      <td>User session created - session_id: sess_665158</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-12-24T15:22:22.602903Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>payment-service</td>\n",
       "      <td>API request received - endpoint: /api/payments...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-12-24T15:22:22.621293Z</td>\n",
       "      <td>WARN</td>\n",
       "      <td>notification-service</td>\n",
       "      <td>Rate limit approaching for user 9201</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-12-24T15:22:22.679093Z</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Cache miss for key: cache_94</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-12-24T15:22:22.714181Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>notification-service</td>\n",
       "      <td>Cache hit for key: cache_58</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-12-24T15:22:22.755490Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Email notification sent to user249@example.com</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp  level               service  \\\n",
       "0  2025-12-24T15:22:22.408389Z  DEBUG          auth-service   \n",
       "1  2025-12-24T15:22:22.445143Z   WARN           api-gateway   \n",
       "2  2025-12-24T15:22:22.476306Z   INFO  notification-service   \n",
       "3  2025-12-24T15:22:22.521972Z  DEBUG          user-service   \n",
       "4  2025-12-24T15:22:22.565355Z   INFO         order-service   \n",
       "5  2025-12-24T15:22:22.602903Z   INFO       payment-service   \n",
       "6  2025-12-24T15:22:22.621293Z   WARN  notification-service   \n",
       "7  2025-12-24T15:22:22.679093Z  DEBUG           api-gateway   \n",
       "8  2025-12-24T15:22:22.714181Z   INFO  notification-service   \n",
       "9  2025-12-24T15:22:22.755490Z   INFO           api-gateway   \n",
       "\n",
       "                                             message  is_anomaly anomaly_type  \n",
       "0                       Cache miss for key: cache_70           0         None  \n",
       "1              Slow query detected - duration: 184ms           0         None  \n",
       "2  Database query completed - rows: 297, duration...           0         None  \n",
       "3                       Cache miss for key: cache_89           0         None  \n",
       "4     User session created - session_id: sess_665158           0         None  \n",
       "5  API request received - endpoint: /api/payments...           0         None  \n",
       "6               Rate limit approaching for user 9201           0         None  \n",
       "7                       Cache miss for key: cache_94           0         None  \n",
       "8                        Cache hit for key: cache_58           0         None  \n",
       "9     Email notification sent to user249@example.com           0         None  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows.\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Normal Logs:\")\n",
    "print(\"=\"*60)\n",
    "df[df['is_anomaly'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Sample Anomalous Logs:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>level</th>\n",
       "      <th>service</th>\n",
       "      <th>message</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>anomaly_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>2025-12-24T15:22:44.729397Z</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>order-service</td>\n",
       "      <td>OutOfMemoryError - heap space exceeded</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>2025-12-24T15:23:00.956047Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>user-service</td>\n",
       "      <td>Data corruption detected in table users</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>2025-12-24T15:23:09.198228Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>inventory-service</td>\n",
       "      <td>NullPointerException in updateInventory at lin...</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>2025-12-24T15:23:12.983577Z</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>notification-service</td>\n",
       "      <td>Unexpected error code E2501 from external API</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>2025-12-24T15:23:52.745656Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>payment-service</td>\n",
       "      <td>StackOverflowError in updateInventory</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344</th>\n",
       "      <td>2025-12-24T15:24:28.927695Z</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>auth-service</td>\n",
       "      <td>Data corruption detected in table orders</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>2025-12-24T15:25:33.075876Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>auth-service</td>\n",
       "      <td>NullPointerException in validateUser at line 487</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>2025-12-24T15:26:40.801193Z</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>notification-service</td>\n",
       "      <td>Unexpected error code E3785 from external API</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7285</th>\n",
       "      <td>2025-12-24T15:26:58.137832Z</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>user-service</td>\n",
       "      <td>Unexpected error code E7441 from external API</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7960</th>\n",
       "      <td>2025-12-24T15:27:23.697186Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>auth-service</td>\n",
       "      <td>Unexpected error code E9127 from external API</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        timestamp  level               service  \\\n",
       "589   2025-12-24T15:22:44.729397Z  FATAL         order-service   \n",
       "1019  2025-12-24T15:23:00.956047Z  ERROR          user-service   \n",
       "1237  2025-12-24T15:23:09.198228Z  ERROR     inventory-service   \n",
       "1338  2025-12-24T15:23:12.983577Z  FATAL  notification-service   \n",
       "2389  2025-12-24T15:23:52.745656Z  ERROR       payment-service   \n",
       "3344  2025-12-24T15:24:28.927695Z  FATAL          auth-service   \n",
       "5038  2025-12-24T15:25:33.075876Z  ERROR          auth-service   \n",
       "6826  2025-12-24T15:26:40.801193Z  FATAL  notification-service   \n",
       "7285  2025-12-24T15:26:58.137832Z  FATAL          user-service   \n",
       "7960  2025-12-24T15:27:23.697186Z  ERROR          auth-service   \n",
       "\n",
       "                                                message  is_anomaly  \\\n",
       "589              OutOfMemoryError - heap space exceeded           1   \n",
       "1019            Data corruption detected in table users           1   \n",
       "1237  NullPointerException in updateInventory at lin...           1   \n",
       "1338      Unexpected error code E2501 from external API           1   \n",
       "2389              StackOverflowError in updateInventory           1   \n",
       "3344           Data corruption detected in table orders           1   \n",
       "5038   NullPointerException in validateUser at line 487           1   \n",
       "6826      Unexpected error code E3785 from external API           1   \n",
       "7285      Unexpected error code E7441 from external API           1   \n",
       "7960      Unexpected error code E9127 from external API           1   \n",
       "\n",
       "     anomaly_type  \n",
       "589   new_pattern  \n",
       "1019  new_pattern  \n",
       "1237  new_pattern  \n",
       "1338  new_pattern  \n",
       "2389  new_pattern  \n",
       "3344  new_pattern  \n",
       "5038  new_pattern  \n",
       "6826  new_pattern  \n",
       "7285  new_pattern  \n",
       "7960  new_pattern  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display anomalous logs.\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Anomalous Logs:\")\n",
    "print(\"=\"*60)\n",
    "df[df['is_anomaly'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED DATASET STATISTICS\n",
      "============================================================\n",
      "\n",
      "Class Distribution:\n",
      "is_anomaly\n",
      "0    95000\n",
      "1     1237\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Anomaly Rate: 1.29%\n",
      "\n",
      "\n",
      "Log Level Distribution:\n",
      "level\n",
      "INFO     61864\n",
      "DEBUG    18814\n",
      "WARN      9885\n",
      "ERROR     4756\n",
      "FATAL      918\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Log Level Distribution by Anomaly Status:\n",
      "       Normal  Anomaly  Total\n",
      "level                        \n",
      "DEBUG   18814        0  18814\n",
      "ERROR    4294      462   4756\n",
      "FATAL     488      430    918\n",
      "INFO    61864        0  61864\n",
      "WARN     9540      345   9885\n",
      "All     95000     1237  96237\n",
      "\n",
      "------------------------------------------------------------\n",
      "KEY INSIGHT: Expected Errors in Normal Logs\n",
      "------------------------------------------------------------\n",
      "Expected ERROR/FATAL logs (NOT anomalies): 4,782\n",
      "These are business errors like:\n",
      "  - Invalid passwords\n",
      "  - Insufficient funds\n",
      "  - Missing fields\n",
      "  - Session expired\n",
      "\n",
      "\n",
      "Anomaly Type Distribution:\n",
      "anomaly_type\n",
      "spike_error_spike    447\n",
      "cascade_failure      232\n",
      "resource_spike       217\n",
      "cascade_spike        170\n",
      "new_pattern          100\n",
      "security_breach       71\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Service Distribution:\n",
      "service\n",
      "api-gateway             14122\n",
      "user-service            13839\n",
      "auth-service            13715\n",
      "payment-service         13698\n",
      "order-service           13696\n",
      "notification-service    13645\n",
      "inventory-service       13522\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Detailed statistics.\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED DATASET STATISTICS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(df['is_anomaly'].value_counts())\n",
    "print(f\"\\nAnomaly Rate: {(df['is_anomaly'].sum() / len(df)) * 100:.2f}%\\n\")\n",
    "\n",
    "print(\"\\nLog Level Distribution:\")\n",
    "print(df['level'].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"\\nLog Level Distribution by Anomaly Status:\")\n",
    "level_anomaly = pd.crosstab(df['level'], df['is_anomaly'], margins=True)\n",
    "level_anomaly.columns = ['Normal', 'Anomaly', 'Total']\n",
    "print(level_anomaly)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"KEY INSIGHT: Expected Errors in Normal Logs\")\n",
    "print(\"-\"*60)\n",
    "normal_errors = df[(df['is_anomaly'] == 0) & (df['level'].isin(['ERROR', 'FATAL']))]\n",
    "print(f\"Expected ERROR/FATAL logs (NOT anomalies): {len(normal_errors):,}\")\n",
    "print(f\"These are business errors like:\")\n",
    "print(\"  - Invalid passwords\")\n",
    "print(\"  - Insufficient funds\")\n",
    "print(\"  - Missing fields\")\n",
    "print(\"  - Session expired\")\n",
    "print()\n",
    "\n",
    "print(\"\\nAnomaly Type Distribution:\")\n",
    "if 'anomaly_type' in df.columns:\n",
    "    anomaly_dist = df[df['is_anomaly'] == 1]['anomaly_type'].value_counts()\n",
    "    print(anomaly_dist)\n",
    "    print()\n",
    "\n",
    "print(\"\\nService Distribution:\")\n",
    "print(df['service'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training data saved to: ../data/training_logs.csv\n",
      "  File size: 8.97 MB\n",
      "\n",
      "✓ Training data saved to: ../data/training_logs.parquet\n",
      "  File size: 2.19 MB\n"
     ]
    }
   ],
   "source": [
    "# Create data directory if it doesn't exist.\n",
    "import os\n",
    "\n",
    "data_dir = '../data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# # Save to CSV.\n",
    "# output_file = os.path.join(data_dir, 'training_logs.csv')\n",
    "# df.to_csv(output_file, index=False)\n",
    "# print(f\"\\n✓ Training data saved to: {output_file}\")\n",
    "# print(f\"  File size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Try to save as Parquet for better performance (optional).\n",
    "try:\n",
    "    output_parquet = os.path.join(data_dir, 'training_logs.parquet')\n",
    "    df.to_parquet(output_parquet, index=False, engine='fastparquet')\n",
    "    print(f\"\\n✓ Training data saved to: {output_parquet}\")\n",
    "    print(f\"  File size: {os.path.getsize(output_parquet) / (1024*1024):.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Parquet save skipped (pyarrow/fastparquet not available)\")\n",
    "    print(f\"  CSV format is sufficient for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Streaming Test Data (Smaller Sample)\n",
    "\n",
    "Generate a smaller dataset for testing the streaming pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING TEST DATASET\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "REALISTIC LOG DATA GENERATION\n",
      "============================================================\n",
      "\n",
      "Generating logs over 10 minute window...\n",
      "Normal logs (baseline): 9,500\n",
      "Anomaly patterns to inject:\n",
      "  - Error spikes: 3\n",
      "  - Cascade failures: 1\n",
      "  - New error patterns: 20\n",
      "  - Security incidents: 1\n",
      "\n",
      "Generating baseline traffic...\n",
      "✓ Baseline complete: 9,500 logs\n",
      "\n",
      "Injecting 3 error spikes...\n",
      "  Spike 1: 72 errors at 16:17:25 from order-service\n",
      "  Spike 2: 32 errors at 16:17:25 from payment-service\n",
      "  Spike 3: 43 errors at 16:17:25 from user-service\n",
      "✓ Spikes injected\n",
      "\n",
      "Injecting 1 cascade failures...\n",
      "  Cascade 1: 59 errors across 3 services at 16:12:28\n",
      "✓ Cascades injected\n",
      "\n",
      "Injecting 20 new error patterns...\n",
      "✓ New patterns injected\n",
      "\n",
      "Injecting 1 security incidents...\n",
      "  Security incident 1 at 16:17:25\n",
      "✓ Security incidents injected\n",
      "\n",
      "Creating dataset...\n",
      "\n",
      "============================================================\n",
      "DATASET GENERATED SUCCESSFULLY\n",
      "============================================================\n",
      "Total logs: 9,748\n",
      "Normal logs: 9,500 (97.46%)\n",
      "Anomalous logs: 248 (2.54%)\n",
      "\n",
      "Key insight: Not all ERROR/FATAL logs are anomalies!\n",
      "  - Baseline contains 498 expected errors\n",
      "  - Anomalies are identified by PATTERNS (spikes, cascades, new errors)\n",
      "\n",
      "\n",
      "✓ Test data saved to: ../data/test_logs.csv\n",
      "  Total logs: 9,748\n",
      "  Anomaly rate: 2.54%\n"
     ]
    }
   ],
   "source": [
    "# Generate smaller test set for streaming validation.\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING TEST DATASET\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "test_df = generate_training_data(\n",
    "    num_normal=9500,\n",
    "    duration_minutes=10,\n",
    "    num_spikes=3,\n",
    "    num_cascades=1,\n",
    "    num_new_patterns=20,\n",
    "    num_security_incidents=1\n",
    ")\n",
    "\n",
    "# Save test data.\n",
    "test_output = os.path.join(data_dir, 'test_logs.csv')\n",
    "test_df.to_csv(test_output, index=False)\n",
    "print(f\"\\n✓ Test data saved to: {test_output}\")\n",
    "print(f\"  Total logs: {len(test_df):,}\")\n",
    "print(f\"  Anomaly rate: {(test_df['is_anomaly'].sum() / len(test_df)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook generates **realistic** synthetic log data with:\n",
    "\n",
    "### 1. **Realistic Normal Logs (Including Expected Errors)**\n",
    "- **INFO/DEBUG/WARN**: Typical operations (requests, queries, cache operations).\n",
    "- **Expected ERROR logs** (NOT anomalies):\n",
    "  - Invalid passwords, missing fields, validation errors.\n",
    "  - Business logic errors (insufficient funds, out of stock).\n",
    "  - Session expired, rate limiting.\n",
    "- **Expected FATAL logs** (NOT anomalies when isolated):\n",
    "  - Rare unhandled exceptions.\n",
    "\n",
    "### 2. **True Anomaly Patterns**\n",
    "Anomalies are detected by **patterns**, not just log level:\n",
    "\n",
    "- **Error Spikes**: Sudden burst of errors (30-80 errors in seconds).\n",
    "- **Cascade Failures**: Multiple services failing together.\n",
    "- **New Error Patterns**: Unusual errors that rarely occur (NullPointer, OutOfMemory).\n",
    "- **Resource Exhaustion**: Critical resource issues (disk full, memory critical).\n",
    "- **Security Incidents**: Brute force attacks, suspicious activity patterns.\n",
    "\n",
    "### 3. **Key Differences from Traditional Approach**\n",
    "- ❌ **Wrong**: All ERROR/FATAL logs = Anomalies.\n",
    "- ✅ **Correct**: Anomalies = Unusual **patterns** and **behavior**.\n",
    "  \n",
    "### 4. **What Makes This Realistic**\n",
    "- Expected errors occur naturally in baseline traffic.\n",
    "- Anomalies are temporal patterns (spikes, sequences, new behaviors).\n",
    "- Mixed log levels in both normal and anomalous data.\n",
    "- Time-series structure with realistic timestamps.\n",
    "\n",
    "### 5. **Model Training Implications**\n",
    "Your anomaly detection model should learn:\n",
    "- **Frequency patterns**: Error rate vs time.\n",
    "- **Temporal sequences**: Spike detection, cascade patterns.\n",
    "- **Novelty detection**: New error messages.\n",
    "- **Service correlation**: Multiple services failing together.\n",
    "- **NOT just**: log_level == ERROR → anomaly.\n",
    "\n",
    "### 6. **Output Files**\n",
    "- `training_logs.csv`: Full training dataset (~96k logs).\n",
    "- `training_logs.parquet`: Same data, compressed format.\n",
    "- `test_logs.csv`: Smaller test set for pipeline validation.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Train models with temporal features (sliding windows, error rates).\n",
    "2. Use anomaly_type field for multi-class classification.\n",
    "3. Test streaming pipeline with realistic patterns.\n",
    "4. Tune detection for < 30 second latency requirement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
