{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility.\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can generate 1l row maybe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log generation configuration.\n",
    "NUM_DAYS = 9  # Generate data over 9 days.\n",
    "LOGS_PER_DAY = 15000  # Average logs per day.\n",
    "TOTAL_NORMAL_LOGS = NUM_DAYS * LOGS_PER_DAY  # Total normal logs.\n",
    "\n",
    "# Anomaly configuration - spread over 3-4 day periods randomly.\n",
    "TOTAL_SPIKES = 8  # Total spikes across 9 days (not evenly distributed).\n",
    "TOTAL_CASCADES = 3  # Total cascade failures.\n",
    "TOTAL_SECURITY_INCIDENTS = 2  # Total security incidents.\n",
    "TOTAL_NEW_PATTERNS = 40  # Total new error patterns (not per day).\n",
    "\n",
    "# Services in the system.\n",
    "SERVICES = [\n",
    "    'auth-service',\n",
    "    'payment-service',\n",
    "    'order-service',\n",
    "    'inventory-service',\n",
    "    'user-service',\n",
    "    'notification-service',\n",
    "    'api-gateway'\n",
    "]\n",
    "\n",
    "# Log levels.\n",
    "LOG_LEVELS = ['INFO', 'DEBUG', 'WARN', 'ERROR', 'FATAL']\n",
    "\n",
    "# Normal log level distribution (includes some expected errors).\n",
    "NORMAL_LEVEL_WEIGHTS = [0.65, 0.20, 0.10, 0.045, 0.005]\n",
    "\n",
    "# Anomaly types.\n",
    "ANOMALY_TYPES = ['spike', 'cascade', 'new_pattern', 'resource_exhaustion', 'security_breach']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Log Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal log message templates (including EXPECTED errors that are NOT anomalies).\n",
    "NORMAL_TEMPLATES = {\n",
    "    'INFO': [\n",
    "        'Request processed successfully - user_id: {user_id}, duration: {duration}ms',\n",
    "        'Authentication successful for user {user_id}',\n",
    "        'Payment processed - transaction_id: {transaction_id}, amount: ${amount}',\n",
    "        'Order created successfully - order_id: {order_id}',\n",
    "        'Cache hit for key: {cache_key}',\n",
    "        'Database query completed - rows: {rows}, duration: {duration}ms',\n",
    "        'API request received - endpoint: {endpoint}, method: {method}',\n",
    "        'User session created - session_id: {session_id}',\n",
    "        'Email notification sent to {email}',\n",
    "        'Inventory updated - item_id: {item_id}, quantity: {quantity}',\n",
    "    ],\n",
    "    'DEBUG': [\n",
    "        'Entering function: {function_name}',\n",
    "        'Cache miss for key: {cache_key}',\n",
    "        'Query plan: {query_plan}',\n",
    "        'Response payload size: {size} bytes',\n",
    "        'Connection pool stats: active={active}, idle={idle}',\n",
    "    ],\n",
    "    'WARN': [\n",
    "        'High memory usage detected: {memory}%',\n",
    "        'Slow query detected - duration: {duration}ms',\n",
    "        'Rate limit approaching for user {user_id}',\n",
    "        'Connection retry attempt {attempt} of 3',\n",
    "        'Cache eviction due to size limit',\n",
    "        'Service latency above threshold: {duration}ms',\n",
    "    ],\n",
    "    # EXPECTED ERRORS (business logic, user errors - NOT anomalies).\n",
    "    'ERROR': [\n",
    "        'Authentication failed - invalid password for user {user_id}',\n",
    "        'Validation error - missing required field: {field_name}',\n",
    "        'Order failed - insufficient inventory for item {item_id}',\n",
    "        'Payment declined - insufficient funds for user {user_id}',\n",
    "        'Invalid request - malformed JSON in request body',\n",
    "        'Resource not found - user_id {user_id} does not exist',\n",
    "        'Session expired for user {user_id}',\n",
    "        'Rate limit exceeded for user {user_id} - retry after {retry_after}s',\n",
    "    ],\n",
    "    # RARE but expected critical issues (NOT anomalies when isolated).\n",
    "    'FATAL': [\n",
    "        'Unhandled exception in request handler - {exception_type}',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomalous Log Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalous log message templates (TRUE system anomalies).\n",
    "# These represent UNUSUAL patterns, not just error-level logs.\n",
    "ANOMALY_TEMPLATES = {\n",
    "    # Error spike patterns.\n",
    "    'spike_error': [\n",
    "        'Database connection pool exhausted - max connections: {max_conn}',\n",
    "        'Service unavailable - {service} not responding after {attempts} attempts',\n",
    "        'Connection timeout to {host} - {error_msg}',\n",
    "        'Failed to acquire database lock - deadlock detected',\n",
    "    ],\n",
    "    # Cascade failure patterns.\n",
    "    'cascade': [\n",
    "        'Circuit breaker opened for {service} - failure threshold exceeded',\n",
    "        'Upstream service {service} unavailable - cascading failure',\n",
    "        'Database cluster unreachable - all nodes down',\n",
    "        'Message queue full - dropping messages',\n",
    "    ],\n",
    "    # New/unusual error patterns.\n",
    "    'new_pattern': [\n",
    "        'NullPointerException in {function_name} at line {line}',\n",
    "        'OutOfMemoryError - heap space exceeded',\n",
    "        'StackOverflowError in {function_name}',\n",
    "        'Data corruption detected in table {table_name}',\n",
    "        'Unexpected error code {error_code} from external API',\n",
    "    ],\n",
    "    # Resource exhaustion.\n",
    "    'resource': [\n",
    "        'Disk space critically low: {disk_space}% remaining',\n",
    "        'Memory usage critical: {memory}% used',\n",
    "        'CPU usage sustained above 95% for {duration} seconds',\n",
    "        'File descriptor limit reached',\n",
    "        'Thread pool exhausted - queue size: {queue_size}',\n",
    "    ],\n",
    "    # Security anomalies.\n",
    "    'security': [\n",
    "        'Suspicious activity detected from IP: {ip_address}',\n",
    "        'Brute force attack detected - {attempts} failed login attempts',\n",
    "        'Potential SQL injection attempt in query',\n",
    "        'Unauthorized access attempt to admin endpoint',\n",
    "        'Token validation failed - possible token forgery',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timestamp_from_datetime(dt: datetime) -> str:\n",
    "    \"\"\"\n",
    "    Convert datetime to ISO format timestamp string.\n",
    "\n",
    "    param dt: Datetime object.\n",
    "    \"\"\"\n",
    "    return dt.isoformat() + 'Z'\n",
    "\n",
    "\n",
    "def generate_normal_log(timestamp: datetime, service: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generate a normal log entry (including expected business errors).\n",
    "\n",
    "    param timestamp: Log timestamp as datetime.\n",
    "    param service: Service name.\n",
    "    \"\"\"\n",
    "    level = random.choices(LOG_LEVELS, weights=NORMAL_LEVEL_WEIGHTS)[0]\n",
    "    template = random.choice(NORMAL_TEMPLATES[level])\n",
    "    \n",
    "    # Generate realistic field values.\n",
    "    message = template.format(\n",
    "        user_id=random.randint(1000, 9999),\n",
    "        duration=random.randint(10, 500),\n",
    "        transaction_id=f'txn_{random.randint(100000, 999999)}',\n",
    "        amount=round(random.uniform(10, 1000), 2),\n",
    "        order_id=f'ord_{random.randint(100000, 999999)}',\n",
    "        cache_key=f'cache_{random.randint(1, 100)}',\n",
    "        rows=random.randint(1, 1000),\n",
    "        endpoint=random.choice(['/api/users', '/api/orders', '/api/payments']),\n",
    "        method=random.choice(['GET', 'POST', 'PUT']),\n",
    "        session_id=f'sess_{random.randint(100000, 999999)}',\n",
    "        email=f'user{random.randint(1, 1000)}@example.com',\n",
    "        item_id=f'item_{random.randint(1, 500)}',\n",
    "        quantity=random.randint(1, 100),\n",
    "        function_name=random.choice(['processPayment', 'validateUser', 'updateInventory']),\n",
    "        query_plan='index_scan',\n",
    "        size=random.randint(100, 50000),\n",
    "        active=random.randint(5, 20),\n",
    "        idle=random.randint(10, 50),\n",
    "        memory=random.randint(40, 75),\n",
    "        attempt=random.randint(1, 3),\n",
    "        field_name=random.choice(['email', 'password', 'amount', 'item_id']),\n",
    "        retry_after=random.randint(30, 300),\n",
    "        exception_type=random.choice(['ValueError', 'TypeError', 'KeyError'])\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "        'level': level,\n",
    "        'service': service,\n",
    "        'message': message,\n",
    "        'is_anomaly': 0,\n",
    "        'anomaly_type': None\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_anomaly_spike(start_time: datetime, service: str, spike_size: int = 50) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate a spike of error logs (anomaly pattern).\n",
    "\n",
    "    param start_time: Starting timestamp.\n",
    "    param service: Service experiencing the spike.\n",
    "    param spike_size: Number of errors in the spike.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    anomaly_category = random.choice(['spike_error', 'cascade', 'resource'])\n",
    "    template = random.choice(ANOMALY_TEMPLATES[anomaly_category])\n",
    "    \n",
    "    for i in range(spike_size):\n",
    "        timestamp = start_time + timedelta(seconds=i * random.uniform(0.1, 2))\n",
    "        \n",
    "        message = template.format(\n",
    "            max_conn=random.choice([50, 100, 200]),\n",
    "            service=random.choice(SERVICES),\n",
    "            attempts=random.randint(3, 10),\n",
    "            host=f'db-{random.randint(1, 5)}.example.com',\n",
    "            error_msg=random.choice(['Connection refused', 'Timeout', 'Network unreachable']),\n",
    "            function_name=random.choice(['processPayment', 'validateUser', 'updateInventory']),\n",
    "            line=random.randint(100, 500),\n",
    "            table_name=random.choice(['users', 'orders', 'payments']),\n",
    "            error_code=f'E{random.randint(1000, 9999)}',\n",
    "            disk_space=random.randint(1, 5),\n",
    "            memory=random.randint(90, 99),\n",
    "            duration=random.randint(300, 600),\n",
    "            queue_size=random.randint(10000, 50000),\n",
    "            ip_address=f'{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}'\n",
    "        )\n",
    "        \n",
    "        logs.append({\n",
    "            'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "            'level': random.choice(['ERROR', 'FATAL', 'WARN']),\n",
    "            'service': service,\n",
    "            'message': message,\n",
    "            'is_anomaly': 1,\n",
    "            'anomaly_type': f'{anomaly_category}_spike'\n",
    "        })\n",
    "    \n",
    "    return logs\n",
    "\n",
    "\n",
    "def generate_cascade_failure(start_time: datetime, affected_services: list[str], cascade_size: int = 30) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate cascade failure across multiple services (anomaly pattern).\n",
    "\n",
    "    param start_time: Starting timestamp.\n",
    "    param affected_services: Services affected by cascade.\n",
    "    param cascade_size: Number of logs in cascade.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    templates = ANOMALY_TEMPLATES['cascade']\n",
    "    \n",
    "    for i in range(cascade_size):\n",
    "        timestamp = start_time + timedelta(seconds=i * random.uniform(1, 5))\n",
    "        service = random.choice(affected_services)\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        message = template.format(\n",
    "            service=random.choice(SERVICES)\n",
    "        )\n",
    "        \n",
    "        logs.append({\n",
    "            'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "            'level': random.choice(['ERROR', 'FATAL']),\n",
    "            'service': service,\n",
    "            'message': message,\n",
    "            'is_anomaly': 1,\n",
    "            'anomaly_type': 'cascade_failure'\n",
    "        })\n",
    "    \n",
    "    return logs\n",
    "\n",
    "\n",
    "def generate_new_pattern_anomaly(timestamp: datetime, service: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generate a new/unusual error pattern (anomaly).\n",
    "\n",
    "    param timestamp: Log timestamp.\n",
    "    param service: Service name.\n",
    "    \"\"\"\n",
    "    template = random.choice(ANOMALY_TEMPLATES['new_pattern'])\n",
    "    \n",
    "    message = template.format(\n",
    "        function_name=random.choice(['processPayment', 'validateUser', 'updateInventory']),\n",
    "        line=random.randint(50, 500),\n",
    "        table_name=random.choice(['users', 'orders', 'payments']),\n",
    "        error_code=f'E{random.randint(1000, 9999)}'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "        'level': random.choice(['ERROR', 'FATAL']),\n",
    "        'service': service,\n",
    "        'message': message,\n",
    "        'is_anomaly': 1,\n",
    "        'anomaly_type': 'new_pattern'\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_security_anomaly(start_time: datetime, num_events: int = 20) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generate security-related anomaly (multiple suspicious events).\n",
    "\n",
    "    param start_time: Starting timestamp.\n",
    "    param num_events: Number of security events.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    ip = f'{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}'\n",
    "    \n",
    "    for i in range(num_events):\n",
    "        timestamp = start_time + timedelta(seconds=i * random.uniform(0.5, 3))\n",
    "        template = random.choice(ANOMALY_TEMPLATES['security'])\n",
    "        \n",
    "        message = template.format(\n",
    "            ip_address=ip,\n",
    "            attempts=i + 1\n",
    "        )\n",
    "        \n",
    "        logs.append({\n",
    "            'timestamp': generate_timestamp_from_datetime(timestamp),\n",
    "            'level': 'WARN' if i < num_events - 5 else 'ERROR',\n",
    "            'service': 'auth-service',\n",
    "            'message': message,\n",
    "            'is_anomaly': 1,\n",
    "            'anomaly_type': 'security_breach'\n",
    "        })\n",
    "    \n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(\n",
    "    num_days: int = NUM_DAYS,\n",
    "    logs_per_day: int = LOGS_PER_DAY,\n",
    "    total_spikes: int = TOTAL_SPIKES,\n",
    "    total_cascades: int = TOTAL_CASCADES,\n",
    "    total_new_patterns: int = TOTAL_NEW_PATTERNS,\n",
    "    total_security_incidents: int = TOTAL_SECURITY_INCIDENTS\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate realistic training data over multiple days with randomly distributed anomalies.\n",
    "\n",
    "    param num_days: Number of days to generate data for.\n",
    "    param logs_per_day: Average number of normal logs per day.\n",
    "    param total_spikes: Total error spikes (randomly distributed).\n",
    "    param total_cascades: Total cascade failures (randomly distributed).\n",
    "    param total_new_patterns: Total new error patterns (randomly distributed).\n",
    "    param total_security_incidents: Total security incidents (randomly distributed).\n",
    "    \"\"\"\n",
    "    total_duration_minutes = num_days * 24 * 60\n",
    "    num_normal = num_days * logs_per_day\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"REALISTIC LOG DATA GENERATION - MULTI-DAY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nGenerating logs over {num_days} days...\")\n",
    "    print(f\"Normal logs (baseline): {num_normal:,}\")\n",
    "    print(f\"Anomaly patterns (randomly distributed):\")\n",
    "    print(f\"  - Error spikes: {total_spikes} total\")\n",
    "    print(f\"  - Cascade failures: {total_cascades} total\")\n",
    "    print(f\"  - New error patterns: {total_new_patterns} total\")\n",
    "    print(f\"  - Security incidents: {total_security_incidents} total\")\n",
    "    print(f\"\\nNote: Anomalies spread randomly over 3-4 day periods (if sufficient days).\")\n",
    "    print()\n",
    "    \n",
    "    # Start time is num_days ago.\n",
    "    start_time = datetime.now() - timedelta(days=num_days)\n",
    "    logs = []\n",
    "    \n",
    "    # Generate normal baseline logs with realistic daily patterns.\n",
    "    print(\"Generating baseline traffic with daily patterns...\")\n",
    "    for i in range(num_normal):\n",
    "        # Calculate timestamp with realistic spacing and daily patterns.\n",
    "        avg_interval = (total_duration_minutes * 60) / num_normal\n",
    "        offset = i * avg_interval + random.uniform(-avg_interval * 0.3, avg_interval * 0.3)\n",
    "        timestamp = start_time + timedelta(seconds=offset)\n",
    "        \n",
    "        # Add daily traffic patterns (lower traffic at night).\n",
    "        hour = timestamp.hour\n",
    "        if 2 <= hour <= 6:  # Low traffic at night.\n",
    "            if random.random() < 0.5:  # Skip 50% of logs during night.\n",
    "                continue\n",
    "        elif 9 <= hour <= 17:  # Peak traffic during business hours.\n",
    "            # Add extra logs during peak hours.\n",
    "            if random.random() < 0.2:  # 20% chance of extra log.\n",
    "                service = random.choice(SERVICES)\n",
    "                logs.append(generate_normal_log(timestamp, service))\n",
    "        \n",
    "        service = random.choice(SERVICES)\n",
    "        logs.append(generate_normal_log(timestamp, service))\n",
    "        \n",
    "        if (i + 1) % 30000 == 0:\n",
    "            print(f\"  Generated {i + 1:,} baseline logs...\")\n",
    "    \n",
    "    print(f\"✓ Baseline complete: {len(logs):,} logs (with daily patterns)\\n\")\n",
    "    \n",
    "    # Inject error spikes - cluster them in certain periods if enough days.\n",
    "    print(f\"Injecting {total_spikes} error spikes...\")\n",
    "    \n",
    "    if num_days >= 4:\n",
    "        # Cluster spikes in 2-3 active periods.\n",
    "        print(f\"(Clustered in 2-3 periods)\")\n",
    "        num_active_periods = random.randint(2, 3)\n",
    "        spikes_per_period = [total_spikes // num_active_periods] * num_active_periods\n",
    "        # Distribute remaining spikes.\n",
    "        for i in range(total_spikes % num_active_periods):\n",
    "            spikes_per_period[i] += 1\n",
    "        \n",
    "        spike_count = 0\n",
    "        for period_idx, num_spikes_in_period in enumerate(spikes_per_period):\n",
    "            # Each period spans 3-4 days.\n",
    "            period_start_day = random.randint(0, max(0, num_days - 4))\n",
    "            period_duration_days = min(random.randint(3, 4), num_days - period_start_day)\n",
    "            \n",
    "            print(f\"\\n  Active Period {period_idx + 1}: Days {period_start_day + 1}-{period_start_day + period_duration_days} ({num_spikes_in_period} spikes)\")\n",
    "            \n",
    "            for i in range(num_spikes_in_period):\n",
    "                spike_day = period_start_day + random.randint(0, max(0, period_duration_days - 1))\n",
    "                spike_hour = random.randint(0, 23)\n",
    "                spike_minute = random.randint(0, 59)\n",
    "                spike_time = start_time + timedelta(days=spike_day, hours=spike_hour, minutes=spike_minute)\n",
    "                \n",
    "                service = random.choice(SERVICES)\n",
    "                spike_size = random.randint(30, 100)\n",
    "                spike_logs = generate_anomaly_spike(spike_time, service, spike_size)\n",
    "                logs.extend(spike_logs)\n",
    "                spike_count += 1\n",
    "                print(f\"    Spike {spike_count}: {spike_size} errors on Day {spike_day + 1} at {spike_time.strftime('%Y-%m-%d %H:%M')} from {service}\")\n",
    "    else:\n",
    "        # For short durations, distribute randomly.\n",
    "        print(f\"(Randomly distributed)\")\n",
    "        for i in range(total_spikes):\n",
    "            spike_day = random.randint(0, num_days - 1)\n",
    "            spike_hour = random.randint(0, 23)\n",
    "            spike_minute = random.randint(0, 59)\n",
    "            spike_time = start_time + timedelta(days=spike_day, hours=spike_hour, minutes=spike_minute)\n",
    "            \n",
    "            service = random.choice(SERVICES)\n",
    "            spike_size = random.randint(30, 100)\n",
    "            spike_logs = generate_anomaly_spike(spike_time, service, spike_size)\n",
    "            logs.extend(spike_logs)\n",
    "            print(f\"  Spike {i+1}: {spike_size} errors on Day {spike_day + 1} at {spike_time.strftime('%Y-%m-%d %H:%M')} from {service}\")\n",
    "    \n",
    "    print(f\"\\n✓ Spikes injected\\n\")\n",
    "    \n",
    "    # Inject cascade failures - spread randomly but may cluster.\n",
    "    print(f\"Injecting {total_cascades} cascade failures...\")\n",
    "    for i in range(total_cascades):\n",
    "        cascade_day = random.randint(0, num_days - 1)\n",
    "        cascade_hour = random.randint(0, 23)\n",
    "        cascade_minute = random.randint(0, 59)\n",
    "        cascade_time = start_time + timedelta(days=cascade_day, hours=cascade_hour, minutes=cascade_minute)\n",
    "        \n",
    "        affected = random.sample(SERVICES, k=random.randint(3, 5))\n",
    "        cascade_size = random.randint(40, 80)\n",
    "        cascade_logs = generate_cascade_failure(cascade_time, affected, cascade_size)\n",
    "        logs.extend(cascade_logs)\n",
    "        print(f\"  Cascade {i+1}: {cascade_size} errors on Day {cascade_day+1} at {cascade_time.strftime('%Y-%m-%d %H:%M')} across {len(affected)} services\")\n",
    "    \n",
    "    print(f\"✓ Cascades injected\\n\")\n",
    "    \n",
    "    # Inject new/unusual patterns - spread randomly.\n",
    "    print(f\"Injecting {total_new_patterns} new error patterns (spread randomly)...\")\n",
    "    for i in range(total_new_patterns):\n",
    "        pattern_day = random.randint(0, num_days - 1)\n",
    "        pattern_time = start_time + timedelta(days=pattern_day, hours=random.randint(0, 23), minutes=random.randint(0, 59))\n",
    "        service = random.choice(SERVICES)\n",
    "        logs.append(generate_new_pattern_anomaly(pattern_time, service))\n",
    "    \n",
    "    print(f\"✓ New patterns injected\\n\")\n",
    "    \n",
    "    # Inject security incidents - usually clustered.\n",
    "    print(f\"Injecting {total_security_incidents} security incidents...\")\n",
    "    for i in range(total_security_incidents):\n",
    "        incident_day = random.randint(0, num_days - 1)\n",
    "        incident_time = start_time + timedelta(days=incident_day, hours=random.randint(0, 23), minutes=random.randint(0, 59))\n",
    "        incident_logs = generate_security_anomaly(incident_time, num_events=random.randint(15, 30))\n",
    "        logs.extend(incident_logs)\n",
    "        print(f\"  Security incident {i+1} on Day {incident_day+1} at {incident_time.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    print(f\"✓ Security incidents injected\\n\")\n",
    "    \n",
    "    # Create DataFrame and sort by timestamp.\n",
    "    print(\"Creating dataset...\")\n",
    "    df = pd.DataFrame(logs)\n",
    "    df['timestamp_dt'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp_dt').reset_index(drop=True)\n",
    "    df = df.drop('timestamp_dt', axis=1)\n",
    "    \n",
    "    # Calculate statistics.\n",
    "    total_logs = len(df)\n",
    "    anomalous_logs = df['is_anomaly'].sum()\n",
    "    anomaly_rate = (anomalous_logs / total_logs) * 100\n",
    "    \n",
    "    # Show per-day statistics.\n",
    "    df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "    daily_stats = df.groupby('date').agg({\n",
    "        'is_anomaly': ['count', 'sum']\n",
    "    }).reset_index()\n",
    "    daily_stats.columns = ['Date', 'Total Logs', 'Anomalies']\n",
    "    daily_stats['Anomaly %'] = (daily_stats['Anomalies'] / daily_stats['Total Logs'] * 100).round(2)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATASET GENERATED SUCCESSFULLY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total logs: {total_logs:,}\")\n",
    "    print(f\"Date range: {num_days} days\")\n",
    "    print(f\"Normal logs: {total_logs - anomalous_logs:,} ({100 - anomaly_rate:.2f}%)\")\n",
    "    print(f\"Anomalous logs: {anomalous_logs:,} ({anomaly_rate:.2f}%)\")\n",
    "    print(f\"\\nPer-Day Breakdown:\")\n",
    "    print(daily_stats.to_string(index=False))\n",
    "    print(f\"\\nKey insight: Not all ERROR/FATAL logs are anomalies!\")\n",
    "    print(f\"  - Baseline contains expected errors\")\n",
    "    print(f\"  - Anomalies are identified by PATTERNS (spikes, cascades, new errors)\")\n",
    "    if num_days >= 4:\n",
    "        print(f\"  - Some days have more anomalies, some have fewer (realistic)\")\n",
    "    print()\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REALISTIC LOG DATA GENERATION - MULTI-DAY\n",
      "============================================================\n",
      "\n",
      "Generating logs over 9 days...\n",
      "Normal logs (baseline): 135,000\n",
      "Anomaly patterns (randomly distributed):\n",
      "  - Error spikes: 8 total\n",
      "  - Cascade failures: 3 total\n",
      "  - New error patterns: 40 total\n",
      "  - Security incidents: 2 total\n",
      "\n",
      "Note: Anomalies spread randomly over 3-4 day periods (if sufficient days).\n",
      "\n",
      "Generating baseline traffic with daily patterns...\n",
      "  Generated 30,000 baseline logs...\n",
      "  Generated 60,000 baseline logs...\n",
      "  Generated 90,000 baseline logs...\n",
      "  Generated 120,000 baseline logs...\n",
      "✓ Baseline complete: 131,164 logs (with daily patterns)\n",
      "\n",
      "Injecting 8 error spikes...\n",
      "(Clustered in 2-3 periods)\n",
      "\n",
      "  Active Period 1: Days 5-7 (3 spikes)\n",
      "    Spike 1: 94 errors on Day 6 at 2025-12-23 13:29 from api-gateway\n",
      "    Spike 2: 59 errors on Day 6 at 2025-12-23 06:29 from inventory-service\n",
      "    Spike 3: 69 errors on Day 5 at 2025-12-21 23:52 from inventory-service\n",
      "\n",
      "  Active Period 2: Days 1-3 (3 spikes)\n",
      "    Spike 4: 63 errors on Day 2 at 2025-12-19 02:11 from order-service\n",
      "    Spike 5: 60 errors on Day 1 at 2025-12-18 19:20 from inventory-service\n",
      "    Spike 6: 41 errors on Day 1 at 2025-12-18 09:35 from api-gateway\n",
      "\n",
      "  Active Period 3: Days 1-3 (2 spikes)\n",
      "    Spike 7: 43 errors on Day 3 at 2025-12-20 04:54 from payment-service\n",
      "    Spike 8: 90 errors on Day 2 at 2025-12-19 15:29 from inventory-service\n",
      "\n",
      "✓ Spikes injected\n",
      "\n",
      "Injecting 3 cascade failures...\n",
      "  Cascade 1: 59 errors on Day 9 at 2025-12-26 08:13 across 4 services\n",
      "  Cascade 2: 69 errors on Day 4 at 2025-12-20 23:45 across 3 services\n",
      "  Cascade 3: 51 errors on Day 7 at 2025-12-24 05:55 across 4 services\n",
      "✓ Cascades injected\n",
      "\n",
      "Injecting 40 new error patterns (spread randomly)...\n",
      "✓ New patterns injected\n",
      "\n",
      "Injecting 2 security incidents...\n",
      "  Security incident 1 on Day 1 at 2025-12-18 13:06\n",
      "  Security incident 2 on Day 5 at 2025-12-22 16:55\n",
      "✓ Security incidents injected\n",
      "\n",
      "Creating dataset...\n",
      "\n",
      "============================================================\n",
      "DATASET GENERATED SUCCESSFULLY\n",
      "============================================================\n",
      "Total logs: 131,935\n",
      "Date range: 9 days\n",
      "Normal logs: 131,164 (99.42%)\n",
      "Anomalous logs: 771 (0.58%)\n",
      "\n",
      "Per-Day Breakdown:\n",
      "      Date  Total Logs  Anomalies  Anomaly %\n",
      "2025-12-17         164          0       0.00\n",
      "2025-12-18       14682        123       0.84\n",
      "2025-12-19       14737        159       1.08\n",
      "2025-12-20       14649        115       0.79\n",
      "2025-12-21       14680         79       0.54\n",
      "2025-12-22       14601         18       0.12\n",
      "2025-12-23       14743        158       1.07\n",
      "2025-12-24       14688         55       0.37\n",
      "2025-12-25       14572          3       0.02\n",
      "2025-12-26       14419         61       0.42\n",
      "\n",
      "Key insight: Not all ERROR/FATAL logs are anomalies!\n",
      "  - Baseline contains expected errors\n",
      "  - Anomalies are identified by PATTERNS (spikes, cascades, new errors)\n",
      "  - Some days have more anomalies, some have fewer (realistic)\n",
      "\n",
      "Shape: (131935, 6)\n",
      "Columns: ['timestamp', 'level', 'service', 'message', 'is_anomaly', 'anomaly_type']\n",
      "\n",
      "Date Range: 2025-12-17T23:44:21.030517Z to 2025-12-26T23:44:14.821532Z\n"
     ]
    }
   ],
   "source": [
    "# Generate the training data with realistic anomaly patterns over 9 days.\n",
    "df = generate_training_data()\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nDate Range: {df['timestamp'].min()} to {df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Sample Normal Logs:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>level</th>\n",
       "      <th>service</th>\n",
       "      <th>message</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>anomaly_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-17T23:44:21.030517Z</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>auth-service</td>\n",
       "      <td>Cache miss for key: cache_70</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-17T23:44:26.617160Z</td>\n",
       "      <td>WARN</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Slow query detected - duration: 184ms</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-17T23:44:31.353882Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>notification-service</td>\n",
       "      <td>Database query completed - rows: 297, duration...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-17T23:44:38.295143Z</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>user-service</td>\n",
       "      <td>Cache miss for key: cache_89</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-17T23:44:44.889385Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>order-service</td>\n",
       "      <td>User session created - session_id: sess_665158</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-12-17T23:44:50.596630Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>payment-service</td>\n",
       "      <td>API request received - endpoint: /api/payments...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-12-17T23:44:53.391874Z</td>\n",
       "      <td>WARN</td>\n",
       "      <td>notification-service</td>\n",
       "      <td>Rate limit approaching for user 9201</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-12-17T23:45:02.177521Z</td>\n",
       "      <td>DEBUG</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Cache miss for key: cache_94</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-12-17T23:45:07.510855Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>notification-service</td>\n",
       "      <td>Cache hit for key: cache_58</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-12-17T23:45:13.789790Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Email notification sent to user249@example.com</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp  level               service  \\\n",
       "0  2025-12-17T23:44:21.030517Z  DEBUG          auth-service   \n",
       "1  2025-12-17T23:44:26.617160Z   WARN           api-gateway   \n",
       "2  2025-12-17T23:44:31.353882Z   INFO  notification-service   \n",
       "3  2025-12-17T23:44:38.295143Z  DEBUG          user-service   \n",
       "4  2025-12-17T23:44:44.889385Z   INFO         order-service   \n",
       "5  2025-12-17T23:44:50.596630Z   INFO       payment-service   \n",
       "6  2025-12-17T23:44:53.391874Z   WARN  notification-service   \n",
       "7  2025-12-17T23:45:02.177521Z  DEBUG           api-gateway   \n",
       "8  2025-12-17T23:45:07.510855Z   INFO  notification-service   \n",
       "9  2025-12-17T23:45:13.789790Z   INFO           api-gateway   \n",
       "\n",
       "                                             message  is_anomaly anomaly_type  \n",
       "0                       Cache miss for key: cache_70           0         None  \n",
       "1              Slow query detected - duration: 184ms           0         None  \n",
       "2  Database query completed - rows: 297, duration...           0         None  \n",
       "3                       Cache miss for key: cache_89           0         None  \n",
       "4     User session created - session_id: sess_665158           0         None  \n",
       "5  API request received - endpoint: /api/payments...           0         None  \n",
       "6               Rate limit approaching for user 9201           0         None  \n",
       "7                       Cache miss for key: cache_94           0         None  \n",
       "8                        Cache hit for key: cache_58           0         None  \n",
       "9     Email notification sent to user249@example.com           0         None  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows.\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Normal Logs:\")\n",
    "print(\"=\"*60)\n",
    "df[df['is_anomaly'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Sample Anomalous Logs:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>level</th>\n",
       "      <th>service</th>\n",
       "      <th>message</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>anomaly_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>2025-12-18T03:06:20.548658Z</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>notification-service</td>\n",
       "      <td>StackOverflowError in updateInventory</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>2025-12-18T04:21:20.548658Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>payment-service</td>\n",
       "      <td>StackOverflowError in validateUser</td>\n",
       "      <td>1</td>\n",
       "      <td>new_pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4696</th>\n",
       "      <td>2025-12-18T09:35:20.548658Z</td>\n",
       "      <td>WARN</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Circuit breaker opened for api-gateway - failu...</td>\n",
       "      <td>1</td>\n",
       "      <td>cascade_spike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4697</th>\n",
       "      <td>2025-12-18T09:35:21.202086Z</td>\n",
       "      <td>WARN</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Circuit breaker opened for notification-servic...</td>\n",
       "      <td>1</td>\n",
       "      <td>cascade_spike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4698</th>\n",
       "      <td>2025-12-18T09:35:21.974697Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Circuit breaker opened for user-service - fail...</td>\n",
       "      <td>1</td>\n",
       "      <td>cascade_spike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4699</th>\n",
       "      <td>2025-12-18T09:35:22.411360Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Circuit breaker opened for inventory-service -...</td>\n",
       "      <td>1</td>\n",
       "      <td>cascade_spike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>2025-12-18T09:35:23.290139Z</td>\n",
       "      <td>WARN</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Circuit breaker opened for payment-service - f...</td>\n",
       "      <td>1</td>\n",
       "      <td>cascade_spike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4701</th>\n",
       "      <td>2025-12-18T09:35:24.252878Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Circuit breaker opened for order-service - fai...</td>\n",
       "      <td>1</td>\n",
       "      <td>cascade_spike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>2025-12-18T09:35:25.225362Z</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Circuit breaker opened for inventory-service -...</td>\n",
       "      <td>1</td>\n",
       "      <td>cascade_spike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>2025-12-18T09:35:25.358028Z</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>api-gateway</td>\n",
       "      <td>Circuit breaker opened for notification-servic...</td>\n",
       "      <td>1</td>\n",
       "      <td>cascade_spike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        timestamp  level               service  \\\n",
       "1777  2025-12-18T03:06:20.548658Z  FATAL  notification-service   \n",
       "2172  2025-12-18T04:21:20.548658Z  ERROR       payment-service   \n",
       "4696  2025-12-18T09:35:20.548658Z   WARN           api-gateway   \n",
       "4697  2025-12-18T09:35:21.202086Z   WARN           api-gateway   \n",
       "4698  2025-12-18T09:35:21.974697Z  ERROR           api-gateway   \n",
       "4699  2025-12-18T09:35:22.411360Z  ERROR           api-gateway   \n",
       "4700  2025-12-18T09:35:23.290139Z   WARN           api-gateway   \n",
       "4701  2025-12-18T09:35:24.252878Z  ERROR           api-gateway   \n",
       "4704  2025-12-18T09:35:25.225362Z  FATAL           api-gateway   \n",
       "4705  2025-12-18T09:35:25.358028Z  ERROR           api-gateway   \n",
       "\n",
       "                                                message  is_anomaly  \\\n",
       "1777              StackOverflowError in updateInventory           1   \n",
       "2172                 StackOverflowError in validateUser           1   \n",
       "4696  Circuit breaker opened for api-gateway - failu...           1   \n",
       "4697  Circuit breaker opened for notification-servic...           1   \n",
       "4698  Circuit breaker opened for user-service - fail...           1   \n",
       "4699  Circuit breaker opened for inventory-service -...           1   \n",
       "4700  Circuit breaker opened for payment-service - f...           1   \n",
       "4701  Circuit breaker opened for order-service - fai...           1   \n",
       "4704  Circuit breaker opened for inventory-service -...           1   \n",
       "4705  Circuit breaker opened for notification-servic...           1   \n",
       "\n",
       "       anomaly_type  \n",
       "1777    new_pattern  \n",
       "2172    new_pattern  \n",
       "4696  cascade_spike  \n",
       "4697  cascade_spike  \n",
       "4698  cascade_spike  \n",
       "4699  cascade_spike  \n",
       "4700  cascade_spike  \n",
       "4701  cascade_spike  \n",
       "4704  cascade_spike  \n",
       "4705  cascade_spike  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display anomalous logs.\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Anomalous Logs:\")\n",
    "print(\"=\"*60)\n",
    "df[df['is_anomaly'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED DATASET STATISTICS\n",
      "============================================================\n",
      "\n",
      "Class Distribution:\n",
      "is_anomaly\n",
      "0    131164\n",
      "1       771\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Anomaly Rate: 0.58%\n",
      "\n",
      "\n",
      "Log Level Distribution:\n",
      "level\n",
      "INFO     85358\n",
      "DEBUG    25933\n",
      "WARN     13378\n",
      "ERROR     6295\n",
      "FATAL      971\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Log Level Distribution by Anomaly Status:\n",
      "       Normal  Anomaly   Total\n",
      "level                         \n",
      "DEBUG   25933        0   25933\n",
      "ERROR    6002      293    6295\n",
      "FATAL     696      275     971\n",
      "INFO    85358        0   85358\n",
      "WARN    13175      203   13378\n",
      "All    131164      771  131935\n",
      "\n",
      "------------------------------------------------------------\n",
      "KEY INSIGHT: Expected Errors in Normal Logs\n",
      "------------------------------------------------------------\n",
      "Expected ERROR/FATAL logs (NOT anomalies): 6,698\n",
      "These are business errors like:\n",
      "  - Invalid passwords\n",
      "  - Insufficient funds\n",
      "  - Missing fields\n",
      "  - Session expired\n",
      "\n",
      "\n",
      "Anomaly Type Distribution:\n",
      "anomaly_type\n",
      "cascade_spike        195\n",
      "cascade_failure      179\n",
      "spike_error_spike    171\n",
      "resource_spike       153\n",
      "new_pattern           40\n",
      "security_breach       33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Service Distribution:\n",
      "service\n",
      "api-gateway             19070\n",
      "inventory-service       18966\n",
      "auth-service            18948\n",
      "payment-service         18853\n",
      "order-service           18770\n",
      "user-service            18723\n",
      "notification-service    18605\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Detailed statistics.\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED DATASET STATISTICS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(df['is_anomaly'].value_counts())\n",
    "print(f\"\\nAnomaly Rate: {(df['is_anomaly'].sum() / len(df)) * 100:.2f}%\\n\")\n",
    "\n",
    "print(\"\\nLog Level Distribution:\")\n",
    "print(df['level'].value_counts())\n",
    "print()\n",
    "\n",
    "print(\"\\nLog Level Distribution by Anomaly Status:\")\n",
    "level_anomaly = pd.crosstab(df['level'], df['is_anomaly'], margins=True)\n",
    "level_anomaly.columns = ['Normal', 'Anomaly', 'Total']\n",
    "print(level_anomaly)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"KEY INSIGHT: Expected Errors in Normal Logs\")\n",
    "print(\"-\"*60)\n",
    "normal_errors = df[(df['is_anomaly'] == 0) & (df['level'].isin(['ERROR', 'FATAL']))]\n",
    "print(f\"Expected ERROR/FATAL logs (NOT anomalies): {len(normal_errors):,}\")\n",
    "print(f\"These are business errors like:\")\n",
    "print(\"  - Invalid passwords\")\n",
    "print(\"  - Insufficient funds\")\n",
    "print(\"  - Missing fields\")\n",
    "print(\"  - Session expired\")\n",
    "print()\n",
    "\n",
    "print(\"\\nAnomaly Type Distribution:\")\n",
    "if 'anomaly_type' in df.columns:\n",
    "    anomaly_dist = df[df['is_anomaly'] == 1]['anomaly_type'].value_counts()\n",
    "    print(anomaly_dist)\n",
    "    print()\n",
    "\n",
    "print(\"\\nService Distribution:\")\n",
    "print(df['service'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Training data saved to: ../data/training_logs.parquet\n",
      "  File size: 3.23 MB\n"
     ]
    }
   ],
   "source": [
    "# Create data directory if it doesn't exist.\n",
    "import os\n",
    "\n",
    "data_dir = '../data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# # Save to CSV.\n",
    "# output_file = os.path.join(data_dir, 'training_logs.csv')\n",
    "# df.to_csv(output_file, index=False)\n",
    "# print(f\"\\n✓ Training data saved to: {output_file}\")\n",
    "# print(f\"  File size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Try to save as Parquet for better performance (optional).\n",
    "try:\n",
    "    output_parquet = os.path.join(data_dir, 'training_logs.parquet')\n",
    "    df.to_parquet(output_parquet, index=False, engine='fastparquet')\n",
    "    print(f\"\\n✓ Training data saved to: {output_parquet}\")\n",
    "    print(f\"  File size: {os.path.getsize(output_parquet) / (1024*1024):.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Parquet save skipped (pyarrow/fastparquet not available)\")\n",
    "    print(f\"  CSV format is sufficient for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Streaming Test Data (Smaller Sample)\n",
    "\n",
    "Generate a smaller dataset for testing the streaming pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING TEST DATASET (2 DAYS)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "REALISTIC LOG DATA GENERATION - MULTI-DAY\n",
      "============================================================\n",
      "\n",
      "Generating logs over 2 days...\n",
      "Normal logs (baseline): 10,000\n",
      "Anomaly patterns (randomly distributed):\n",
      "  - Error spikes: 4 total\n",
      "  - Cascade failures: 1 total\n",
      "  - New error patterns: 10 total\n",
      "  - Security incidents: 1 total\n",
      "\n",
      "Note: Anomalies spread randomly over 3-4 day periods (if sufficient days).\n",
      "\n",
      "Generating baseline traffic with daily patterns...\n",
      "✓ Baseline complete: 9,705 logs (with daily patterns)\n",
      "\n",
      "Injecting 4 error spikes...\n",
      "(Randomly distributed)\n",
      "  Spike 1: 32 errors on Day 1 at 2025-12-25 19:46 from api-gateway\n",
      "  Spike 2: 67 errors on Day 2 at 2025-12-26 12:37 from order-service\n",
      "  Spike 3: 80 errors on Day 2 at 2025-12-26 00:04 from notification-service\n",
      "  Spike 4: 68 errors on Day 2 at 2025-12-26 14:37 from api-gateway\n",
      "\n",
      "✓ Spikes injected\n",
      "\n",
      "Injecting 1 cascade failures...\n",
      "  Cascade 1: 55 errors on Day 1 at 2025-12-25 12:52 across 5 services\n",
      "✓ Cascades injected\n",
      "\n",
      "Injecting 10 new error patterns (spread randomly)...\n",
      "✓ New patterns injected\n",
      "\n",
      "Injecting 1 security incidents...\n",
      "  Security incident 1 on Day 2 at 2025-12-26 08:01\n",
      "✓ Security incidents injected\n",
      "\n",
      "Creating dataset...\n",
      "\n",
      "============================================================\n",
      "DATASET GENERATED SUCCESSFULLY\n",
      "============================================================\n",
      "Total logs: 10,045\n",
      "Date range: 2 days\n",
      "Normal logs: 9,705 (96.62%)\n",
      "Anomalous logs: 340 (3.38%)\n",
      "\n",
      "Per-Day Breakdown:\n",
      "      Date  Total Logs  Anomalies  Anomaly %\n",
      "2025-12-24          56          1       1.79\n",
      "2025-12-25        4959         90       1.81\n",
      "2025-12-26        5030        249       4.95\n",
      "\n",
      "Key insight: Not all ERROR/FATAL logs are anomalies!\n",
      "  - Baseline contains expected errors\n",
      "  - Anomalies are identified by PATTERNS (spikes, cascades, new errors)\n",
      "\n",
      "\n",
      "✓ Test data saved to: ../data/test_logs.csv\n",
      "  Total logs: 10,045\n",
      "  Anomaly rate: 3.38%\n"
     ]
    }
   ],
   "source": [
    "# Generate smaller test set for streaming validation (2 days).\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING TEST DATASET (2 DAYS)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "test_df = generate_training_data(\n",
    "    num_days=2,\n",
    "    logs_per_day=5000,\n",
    "    total_spikes=4,\n",
    "    total_cascades=1,\n",
    "    total_new_patterns=10,\n",
    "    total_security_incidents=1\n",
    ")\n",
    "\n",
    "# Save test data.\n",
    "test_output = os.path.join(data_dir, 'test_logs.csv')\n",
    "test_df.to_csv(test_output, index=False)\n",
    "print(f\"\\n✓ Test data saved to: {test_output}\")\n",
    "print(f\"  Total logs: {len(test_df):,}\")\n",
    "print(f\"  Anomaly rate: {(test_df['is_anomaly'].sum() / len(test_df)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log-guard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
